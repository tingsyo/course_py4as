{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 環保署各測站的 PM2.5 資料\n",
    "\n",
    "這個 jupyter notebook 提供了工具和範例，把環保署測站資料的 PM2.5 集合在一起，每個欄位是一個測站的時間序列。在使用之前，請先下載 2015 年環保署所有測站資料，並解壓縮放在 `../data/104_HOUR_00_20160323` 中。\n",
    "\n",
    "首先，我們先載入需要用到的函式庫，以及在「資料清理」課程中使用到的一些函數："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re, os\n",
    "\n",
    "def detect_epa_nan(x):\n",
    "    ''' Search for missing value symbol and assign np.nan '''\n",
    "    if re.findall('\\#|\\*|x', str(x))!=[]:\n",
    "        return(np.nan)\n",
    "    else:\n",
    "        return(x)\n",
    "\n",
    "def detect_epa_norain(x):\n",
    "    ''' Replace 'NR' (no-rain) with 0 '''\n",
    "    if str(x)=='NR':\n",
    "        return(0)\n",
    "    else:\n",
    "        return(x)\n",
    "\n",
    "def clean_epa_station(x):\n",
    "    ''' Clean up a EPA station dataset '''\n",
    "    # Rename columns\n",
    "    col_names = ['date','station','item','h00','h01','h02','h03','h04','h05','h06','h07','h08','h09',\n",
    "                'h10','h11','h12','h13','h14','h15','h16','h17','h18','h19','h20','h21','h22','h23']\n",
    "    x.columns = col_names\n",
    "    # Process NA and NR\n",
    "    floatdata = x.iloc[:,3:]\n",
    "    floatdata = floatdata.applymap(detect_epa_nan)\n",
    "    floatdata = floatdata.applymap(detect_epa_norain)\n",
    "    floatdata.astype(np.float32)\n",
    "    x.iloc[:,3:] = floatdata\n",
    "    # Done\n",
    "    return(x)\n",
    "\n",
    "# Retrieve one item from EPA data and form a time series\n",
    "def retrieve_epa_item(data, var):\n",
    "    tmp = data.loc[data['item']==var,:]\n",
    "    ts = pd.melt(tmp, id_vars=['date'], value_vars=tmp.keys()[3:], var_name='hour', value_name=var)\n",
    "    ts[var] = ts[var].astype(np.float32)\n",
    "    return(ts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 尋找目錄下的所有檔案\n",
    "\n",
    "由於環保署資料包含了很多個檔案，又依照空品區的劃分，放在不同的資料夾底下，因此我們需要借助 python 的 [`os.walk()`](https://docs.python.org/3/library/os.html) 函數，幫我們自動「走遍」資料夾底下的每個檔案。\n",
    "\n",
    "此外，由於資料的檔名包含了測站名稱，我們可以順便把測站名稱取出來，作為欄位的名稱，所以我們需要用到 [`str.find()`](https://www.tutorialspoint.com/python/string_find.htm) 函數，來幫我們找到測站名稱在檔名裡的位置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Walk through the specified path and find all files ended with 'xls'\n",
    "def find_xls_files(path):\n",
    "    ids = []    # 測站名稱\n",
    "    urls = []   # 檔案完整路徑\n",
    "    for root, dirs, files in os.walk(path):             # os.walk() 會傳回「根目錄」、「路徑」、和「檔名」三個字串 list\n",
    "        for fname in files:                             # 每個檔名\n",
    "            if(fname.endswith(\".xls\")):                 # 如果是以 xls 結尾\n",
    "                fid = fname[(fname.find('104年')+4):][:(fname.find('_2016')-5)]\n",
    "                ids.append(fid)\n",
    "                urls.append(os.path.join(root, fname))\n",
    "    return((ids,urls))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由於環保署資料的檔名是以 *YYY年XX站_YYYYMMDD.xls* 的格式命名 ，例如：104年花蓮站_20160320.xls，我們要取出字串裡的 *XX*，所以我們要：\n",
    "\n",
    "1. 從完整檔名裡找到 *'104年'* 之後的字串 A\n",
    "2. 從 A 裡找到 *'_2016'* 前面的字串，不包含「站」\n",
    "\n",
    "因此，我們的寫法會是："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "花蓮站_20160320.xls\n",
      "花蓮\n"
     ]
    }
   ],
   "source": [
    "fn = '104年花蓮站_20160320.xls'\n",
    "fn1 = fn[(fn.find('104年')+4):]\n",
    "print(fn1)\n",
    "fn2 = fn1[:(fn1.find('_2016')-1)]\n",
    "print(fn2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "ids, epafiles = find_xls_files('../data/104_HOUR_00_20160323/')\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 實際進行資料的讀取、清理與合併\n",
    "\n",
    "接下來，我們就用上面的函數，讀進各個測站的資料，清理之後，抽出 PM2.5 的資料，然後用日期跟時間的欄位加以合併。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-ba4e9ccc99e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Collect data from all files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepafiles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m                                         \u001b[0;31m# Start from the first file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m                                  \u001b[0;31m# Read in file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean_epa_station\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m)\u001b[0m                            \u001b[0;31m# Clean up nan and no-rain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mretrieve_epa_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'PM2.5'\u001b[0m\u001b[0;34m)\u001b[0m                  \u001b[0;31m# Retrieve PM2.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# Collect data from all files\n",
    "f = epafiles[0]                                         # Start from the first file\n",
    "tmp = pd.read_excel(f)                                  # Read in file\n",
    "tmp = clean_epa_station(tmp)                            # Clean up nan and no-rain\n",
    "data = retrieve_epa_item(tmp, 'PM2.5')                  # Retrieve PM2.5\n",
    "nancounts = []\n",
    "for f in epafiles[1:]:\n",
    "    tmp = pd.read_excel(f)                                  # Read in file\n",
    "    tmp = clean_epa_station(tmp)                            # Clean up nan and no-rain\n",
    "    tmp = retrieve_epa_item(tmp, 'PM2.5')                   # Retrieve PM2.5\n",
    "    data = data.merge(tmp, on=['date','hour'], how='left')  # Aggregate data\n",
    "    nancounts.append(tmp.apply(lambda x: len(x)-x.count())) # Count the number of nan\n",
    "    #print(f + \": \" + str(len(tmp)))\n",
    "\n",
    "data = data.sort_values(['date', 'hour'])                   # Sort data\n",
    "cnames = list(data.columns[:2]) + ids\n",
    "data.columns = cnames\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 資料的輸出\n",
    "\n",
    "Python 的輸出/輸入非常多樣，我們這裡介紹 pandas.DataFrame 內建的輸出/輸入工具。在我們讀取資料的時候，使用了 [pandas.read_csv()](https://pandas.pydata.org/pandas-docs/stable/io.html#io-read-csv-table) 與 [pandas.read_excel()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_excel.html)，想當然耳，pandas 也提供了輸出成這兩種格式的工具 [pandas.DataFrame.to_csv()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_csv.html) / [pandas.DataFrame.to_excel()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_excel.html)。\n",
    "\n",
    "對於「表格式」的資料來說，CSV 大概是最常用的資料格式，下面我們就把整理好的資料輸出成 csv 檔，方便後續的分析來使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('../data/pm25_2015.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
